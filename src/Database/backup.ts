// backup.js
import { exec } from 'child_process';
import AWS from 'aws-sdk';
import fs from 'fs';
import dayjs from 'dayjs';
import path from 'path';

// AWS S3 ì„¤ì •
const s3 = new AWS.S3({
  accessKeyId: process.env.AWS_ACCESS_KEY,
  secretAccessKey: process.env.AWS_KEY,
  region: process.env.AWS_REGION,
});

// í™˜ê²½ë³€ìˆ˜
const DB_URI = process.env.MONGODB_URI;
const BACKUP_DIR = path.join(__dirname, 'backup');
const DATE_STR = dayjs().format('YYYY-MM-DD_HH-mm-ss');
const BACKUP_NAME = `backup_${DATE_STR}`;
const ARCHIVE_PATH = path.join(BACKUP_DIR, `${BACKUP_NAME}.gz`);

// ë°±ì—… ì‹¤í–‰
async function backupDatabase() {
  try {
    // 1) ë””ë ‰í† ë¦¬ ì¤€ë¹„
    if (!fs.existsSync(BACKUP_DIR)) fs.mkdirSync(BACKUP_DIR);

    // 2) mongodump ì‹¤í–‰ (ì••ì¶•)
    await runCommand(
      `mongodump --uri="${DB_URI}" --archive=${ARCHIVE_PATH} --gzip`,
    );
    console.log('âœ… MongoDB ë°±ì—… ì™„ë£Œ:', ARCHIVE_PATH);

    // 3) S3 ì—…ë¡œë“œ
    const fileStream = fs.createReadStream(ARCHIVE_PATH);
    const uploadParams = {
      Bucket: process.env.S3_BUCKET_NAME, // ì—…ë¡œë“œí•  S3 ë²„í‚·
      Key: `mongodb/${BACKUP_NAME}.gz`,
      Body: fileStream,
    };

    await s3.upload(uploadParams).promise();
    console.log('âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ');

    // 4) ë¡œì»¬ íŒŒì¼ ì‚­ì œ (ì„ íƒ)
    fs.unlinkSync(ARCHIVE_PATH);
    console.log('ðŸ—‘ï¸ ë¡œì»¬ ë°±ì—… íŒŒì¼ ì‚­ì œ ì™„ë£Œ');
  } catch (err) {
    console.error('âŒ ë°±ì—… ì‹¤íŒ¨:', err);
  }
}

// ëª…ë ¹ì–´ ì‹¤í–‰ helper
function runCommand(cmd) {
  return new Promise((resolve, reject) => {
    exec(cmd, (error, stdout, stderr) => {
      if (error) return reject(error);
      console.log(stdout || stderr);
      resolve(true);
    });
  });
}

// ì‹¤í–‰
backupDatabase();
